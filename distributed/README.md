# distributed

This folder contains some resources for getting started using the Spell distributed training feature.

## Distributed model training in PyTorch using DistributedDataParallel <a href="https://web.spell.ml/workspace_create?workspaceName=deeplab-voc-2012&githubUrl=https%3A%2F%2Fgithub.com%2Fspellml%2Fdeeplab-voc-2012"><img src=https://spell.ml/badge.svg height=20px/></a>

![](https://i.imgur.com/Cyq4J0g.png)

This blog post, ["Distributed model training in PyTorch using DistributedDataParallel"](https://spell.ml/blog/pytorch-distributed-data-parallel-XvEaABIAAB8Ars0e), discusses distributed training using the PyTorch-native `DistributedDataParallel` feature.

## Distributed model training using Horovod <a href="https://web.spell.ml/workspace_create?workspaceName=hyper-demo-workspace&githubUrl=https%3A%2F%2Fgithub.com%2Fspellml%2Fexamples"><img src=https://spell.ml/badge.svg height=20px/></a>

This blog post, ["Distributed model training using Horovod"](https://spell.ml/blog/distributed-model-training-using-horovod-XvqEGRUAACgAa5th), discusses distributed training using Uber's [Horovod](https://github.com/horovod/horovod) library.

![](https://i.imgur.com/sWq2UL2.png)
